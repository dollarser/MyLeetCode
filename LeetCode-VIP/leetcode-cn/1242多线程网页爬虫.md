# 1242多线程网页爬虫
<p>给你一个初始地址&nbsp;<code>startUrl</code>&nbsp;和一个 HTML 解析器接口&nbsp;<code>HtmlParser</code>，请你实现一个&nbsp;<strong>多线程的网页爬虫</strong>，用于获取与&nbsp;<code>startUrl</code>&nbsp;有&nbsp;<strong>相同主机名&nbsp;</strong>的所有链接。&nbsp;</p>

<p>以&nbsp;<strong>任意&nbsp;</strong>顺序返回爬虫获取的路径。</p>

<p>爬虫应该遵循：</p>

<ul>
	<li>从&nbsp;<code>startUrl</code>&nbsp;开始</li>
	<li>调用&nbsp;<code>HtmlParser.getUrls(url)</code> 从指定网页路径获得的所有路径。</li>
	<li>不要抓取相同的链接两次。</li>
	<li>仅浏览与&nbsp;<code>startUrl</code>&nbsp;<strong>相同主机名&nbsp;</strong>的链接。</li>
</ul>

<p><img alt="" src="https://user-gold-cdn.xitu.io/2019/11/7/16e463265c7086cb?w=975&amp;h=266&amp;f=png&amp;s=24624" /><img alt="" src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2019/11/09/urlhostname.png" style="height:164px; width:600px" /></p>

<p>如上图所示，主机名是&nbsp;<code>example.org</code>&nbsp;。简单起见，你可以假设所有链接都采用&nbsp;<strong>http 协议</strong>，并且没有指定&nbsp;<strong>端口号</strong>。举个例子，链接&nbsp;<code>http://leetcode.com/problems</code> 和链接&nbsp;<code>http://leetcode.com/contest</code> 属于同一个&nbsp;<strong>主机名</strong>， 而&nbsp;<code>http://example.org/test</code>&nbsp;与&nbsp;<code>http://example.com/abc</code> 并不属于同一个&nbsp;<strong>主机名</strong>。</p>

<p><code>HtmlParser</code> 的接口定义如下：</p>

<pre>
interface HtmlParser {
  // Return a list of all urls from a webpage of given <em>url</em>.
  // This is a blocking call, that means it will do HTTP request and return when this request is finished.
  public List&lt;String&gt; getUrls(String url);
}</pre>

<p>注意一点，<code>getUrls(String url)</code>&nbsp;模拟执行一个HTTP的请求。 你可以将它当做一个阻塞式的方法，直到请求结束。&nbsp;<code>getUrls(String url)</code>&nbsp;保证会在&nbsp;<strong>15ms&nbsp;</strong>内返回所有的路径。 单线程的方案会超过时间限制，你能用多线程方案做的更好吗？</p>

<p>对于问题所需的功能，下面提供了两个例子。为了方便自定义测试，你可以声明三个变量&nbsp;<code>urls</code>，<code>edges</code>&nbsp;和&nbsp;<code>startUrl</code>。但要注意你只能在代码中访问&nbsp;<code>startUrl</code>，并不能直接访问&nbsp;<code>urls</code>&nbsp;和&nbsp;<code>edges</code>。</p>

<p>&nbsp;</p>

<p><strong>拓展问题：</strong></p>

<ol>
	<li>假设我们要要抓取 10000 个节点和 10 亿个路径。并且在每个节点部署相同的的软件。软件可以发现所有的节点。我们必须尽可能减少机器之间的通讯，并确保每个节点负载均衡。你将如何设计这个网页爬虫？</li>
	<li>如果有一个节点发生故障不工作该怎么办？</li>
	<li>如何确认爬虫任务已经完成？</li>
</ol>

<p>&nbsp;</p>

<p><strong>示例 1：</strong></p>

<p><img alt="" src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2019/11/09/sample_2_1497.png" style="height:287px; width:600px" /><img alt="" src="https://user-gold-cdn.xitu.io/2019/11/7/16e46559da0c446a?w=875&amp;h=418&amp;f=png&amp;s=43518" /></p>

<pre>
<strong>输入：
</strong>urls = [
&nbsp; &quot;http://news.yahoo.com&quot;,
&nbsp; &quot;http://news.yahoo.com/news&quot;,
&nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
&nbsp; &quot;http://news.google.com&quot;,
&nbsp; &quot;http://news.yahoo.com/us&quot;
]
edges = [[2,0],[2,1],[3,2],[3,1],[0,4]]
startUrl = &quot;http://news.yahoo.com/news/topics/&quot;
<strong>输出：</strong>[
&nbsp; &quot;http://news.yahoo.com&quot;,
&nbsp; &quot;http://news.yahoo.com/news&quot;,
&nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
&nbsp; &quot;http://news.yahoo.com/us&quot;
]
</pre>

<p><strong>示例 2：</strong></p>

<p><strong><img alt="" src="https://user-gold-cdn.xitu.io/2019/11/7/16e4657b399a5fd2?w=654&amp;h=431&amp;f=png&amp;s=33838" /><img alt="" src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2019/11/09/sample_3_1497.png" style="height:395px; width:530px" /></strong></p>

<pre>
<strong>输入：</strong>
urls = [
&nbsp; &quot;http://news.yahoo.com&quot;,
&nbsp; &quot;http://news.yahoo.com/news&quot;,
&nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
&nbsp; &quot;http://news.google.com&quot;
]
edges = [[0,2],[2,1],[3,2],[3,1],[3,0]]
startUrl = &quot;http://news.google.com&quot;
<strong>输出：</strong>[&quot;http://news.google.com&quot;]
<strong>解释：</strong>startUrl 链接与其他页面不共享一个主机名。</pre>

<p>&nbsp;</p>

<p><strong>提示：</strong></p>

<ul>
	<li><code>1 &lt;= urls.length &lt;= 1000</code></li>
	<li><code>1 &lt;= urls[i].length &lt;= 300</code></li>
	<li><code>startUrl</code>&nbsp;是&nbsp;<code>urls</code>&nbsp;中的一个。</li>
	<li>主机名的长度必须为 1 到 63 个字符（包括点 <code>.</code> 在内），只能包含从 &ldquo;a&rdquo; 到 &ldquo;z&rdquo; 的 ASCII 字母和 &ldquo;0&rdquo; 到 &ldquo;9&rdquo; 的数字，以及中划线 &ldquo;-&rdquo;。</li>
	<li>主机名开头和结尾不能是中划线 &ldquo;-&rdquo;。</li>
	<li>参考资料：<a href="https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames">https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames</a></li>
	<li>你可以假设路径都是不重复的。</li>
</ul>
































# 解题:
# 1.递归调用子线程
Task表示一个任务，url为访问链接，利用HtmlParser得到url对应的所有下一级访问链接；然后对这些子链接进行判断，如果已经访问过了或者主机名不对就continue，将合法的子链接加入新的Task并start，最后用join()等待所有的子Task任务结束再返回。
```java
class Solution {
    class Task extends Thread {
        String url;

        public Task(String url) {
            this.url = url;
        }

        @Override
        public void run() {
            List<Task> subtasks = new ArrayList<>();
            List<String> nextUrls = htmlParser.getUrls(url);
            for (String nextUrl : nextUrls) {
                if (M.putIfAbsent(nextUrl, Boolean.FALSE) != null) continue;//已经访问过
                if (!rootHostname.equals(getHostName(nextUrl))) continue;
                M.put(nextUrl, Boolean.TRUE);
                ans.add(nextUrl);
                Task subTask = new Task(nextUrl);
                subtasks.add(subTask);
                subTask.start();
            }
            for (Task task : subtasks) {
                try {
                    task.join();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        }
    }

    ConcurrentHashMap<String, Boolean> M = new ConcurrentHashMap<>();
    String rootHostname;
    List<String> ans = new ArrayList<>();
    HtmlParser htmlParser;

    static String getHostName(String url) {
        final int start = 7;
        int end = url.indexOf('/', start);
        if (end == -1) {
            end = url.length();
        }
        return url.substring(start, end);
    }

    public List<String> crawl(String startUrl, HtmlParser htmlParser) {
        this.htmlParser = htmlParser;
        this.rootHostname = getHostName(startUrl);
        M.put(startUrl, Boolean.TRUE);
        ans.add(startUrl);
        Thread work = new Task(startUrl);
        work.start();
        try {
            work.join();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        return ans;
    }
}

```

# 2.1242. c++双百100的消息队列解法
![image.png](https://pic.leetcode-cn.com/1636277452-yIavfb-image.png)


### 解题思路
 思路
 - 用经典的线程安全的队列来实现
    - 数据放在一个双端队列里，存在两个，分别是生产和消费
    - 有两个信号量，分别对应生产和消费结果
    - 一个mutex，共同使用
    - 一个同步信号，表示消费者何时可以优雅结束掉
- 至于是否是一个host，我们采用比较简单方式直接获取 http:// 开头到下一个 / 之间内容， 基于偏移量7+下一个/的位置
- 实验结果
   - 并发32 952ms
   - 并发16 572ms
   - 并发8 340ms
   - 并发4 228ms
   - 并发2 168ms 超100
   - 猜测是一个四核机器来跑测试

### 代码

```cpp
/**
 * // This is the HtmlParser's API interface.
 * // You should not implement it, or speculate about its implementation
 * class HtmlParser {
 *   public:
 *     vector<string> getUrls(string url);
 * };
 */
 /*

 */
class Solution {
private:
    string GetHostname(const string& str)
    {
        // http:// 长度是7
        return str.substr(7, str.find("/", 7)-7);
    }

public:
    vector<string> crawl(string startUrl, HtmlParser htmlParser) {
        // 记录结果的set来去重
        unordered_set<string> foundUrls;
        foundUrls.insert(startUrl);

        // hostname 需要都是这个，其他则忽略
        string hostname = GetHostname(startUrl);

        // 多线程的信号量和锁
        mutex mu;
        condition_variable cvReq;
        condition_variable cvRes;
        // 线程同步信息，一旦结束，就退出for循环
        bool isAborted = false;

        // 额外引入一个计数器来判断处理请求的数量，只有完整处理完后才更新
        int reqCnt = 0;
        // 分别代表生产者和消费的结果
        deque<string> requests;
        requests.push_back(startUrl);
        ++reqCnt;
        deque<string> responses;
        
        // 消费者：并行线程启动
        vector<thread> threads;
        int numparallel = 2;
        for (int i = 0; i < numparallel; ++i)
        {
            threads.emplace_back(thread([&]
            {
                while (true)
                {
                    unique_lock<mutex> lk(mu);
                    // 循环等待有请求的信号：等待生产者
                    while (requests.empty() && !isAborted)
                    {
                        cvReq.wait(lk);
                    }
                    if (isAborted)
                    {
                        return;
                    }

                    string currUrl = requests.front();
                    // cout << requests.size() << " " <<  currUrl << endl;
                    requests.pop_front();
                    // 这里减少锁颗粒，先释放
                    lk.unlock();

                    vector<string> nextUrls = htmlParser.getUrls(currUrl);
                    for (string& nextUrl : nextUrls)
                    {
                        // cout << nextUrl << " ";
                        // 只考虑hostname一样的地址
                        if (hostname == GetHostname(nextUrl))
                        {
                            // 插入的时候都需要锁
                            lk.lock();
                            responses.push_back(nextUrl);
                            lk.unlock();
                        }
                    }
                    // cout << endl;

                    --reqCnt;

                    // 通知给生产者
                    cvRes.notify_one();
                }
            }));
        }

        // 生产者：主线程处理
        while (true)
        {
            unique_lock<mutex> lk(mu);
            // 循环等待有消费结果, 或者 请求都处理完了
            while (responses.empty() && reqCnt != 0)
            {
                cvRes.wait(lk);
            }

            // 处理消费结果的数据
            // cout << responses.size() << " ";
            while (!responses.empty())
            {
                string currUrl = responses.front();
                responses.pop_front();
                // 忽略已存在url
                if (foundUrls.find(currUrl) == foundUrls.end())
                {
                    ++reqCnt;
                    requests.push_back(currUrl);
                    foundUrls.insert(currUrl);
                }
            }
            // cout << requests.size() << endl;
            // 这里会通知所有的生产者
            cvReq.notify_all();

            // 如果都处理完了，则结束消费线程
            if (reqCnt == 0)
            {
                isAborted = true;
                break;
            }
        }

        // cout << "end main thread" << endl;
        // 线程结束后 同步并返回结果
        for (int i = 0; i < numparallel; ++i)
        {
            threads[i].join();
        }

        return vector<string>(foundUrls.begin(), foundUrls.end());
    }
};
```
# 3.Java解决方案，使用主循环及多个工作线程。
解题思路简单描述：
- 使用一个ConcurrentHashMap来存储已知的所有URL，主线程向其添加起始URL，多个工作线程可以并发添加已抓取的、符合规则要求的URL。
- 使用一个LinkedList来保存结果URL，使用对应的ReentrantLock来同步多线程访问，结果只增不减。
- 使用一个LinkedList来保存待抓取的URL，使用对应的ReentrantLock来同步多线程访问。主线程添加起始URL，工作线程从该List中取出要开始抓取的URL，抓取后将符合规范的URL添加至该链表及结果URL链表。
- 主循环的退出条件：（1）待抓取URL链表中无元素；（2）已无工作线程（所有的工作线程已完成工作）。
- 需要说明的是，如果待抓取的URL集合很大，那么创建的工作线程会偏多，在生产实践中可以使用线程池来限制并发度。
```
/**
 * // This is the HtmlParser's API interface.
 * // You should not implement it, or speculate about its implementation
 * interface HtmlParser {
 *     public List<String> getUrls(String url) {}
 * }
 */
class Solution {
    // 已知URL集合，存储当前可见的所有URL。
    private ConcurrentHashMap<String, Boolean> totalUrls = new ConcurrentHashMap<>();
    
    // 结果URL链表及对应锁。
    private ReentrantLock resultLock = new ReentrantLock();
    private LinkedList<String> resultUrls = new LinkedList<>();

    // 待抓取URL链表及对应锁。
    private ReentrantLock crawlLock = new ReentrantLock();
    private LinkedList<String> urlsToCrawl = new LinkedList<>();

    // 当前正在执行的工作线程个数。
    private AtomicInteger choreCount = new AtomicInteger(0);
    
    public List<String> crawl(String startUrl, HtmlParser htmlParser) {
        String hostName = extractHostName(startUrl);

        this.totalUrls.put(startUrl, true);

        addUrlToResult(startUrl); 
        addUrlToCrawl(startUrl);

        while (true) {
            String urlToCrawl = fetchUrlToCrawl();
            if (urlToCrawl != null) {
                incrChore();
                Chore chore = new Chore(this, hostName, htmlParser, urlToCrawl);
                (new Thread(chore)).start(); 
            } else {
                if (this.choreCount.get() == 0) {
                    break;
                }
                LockSupport.parkNanos(1L);
            }
        }

        return fetchResultUrls();
    }

    private String extractHostName(String url) {
        // HTTP protocol only.
        String processedUrl = url.substring(7);
        
        int index = processedUrl.indexOf("/");
        if (index == -1) {
            return processedUrl;
        } else {
            return processedUrl.substring(0, index);
        }
    }

    private class Chore implements Runnable {
        private Solution solution;
        private String hostName;
        private HtmlParser htmlParser; 
        private String urlToCrawl;

        public Chore(Solution solution, String hostName, HtmlParser htmlParser, String urlToCrawl) {
            this.solution = solution;
            this.hostName = hostName;
            this.htmlParser = htmlParser;
            this.urlToCrawl = urlToCrawl;
        }

        @Override
        public void run() {
            try {
                filterUrls(this.htmlParser.getUrls(urlToCrawl));
            } finally {
                this.solution.decrChore(); 
            }
        }

        private void filterUrls(List<String> crawledUrls) {
            if (crawledUrls == null || crawledUrls.isEmpty()) {
                return;
            }

            for (String url : crawledUrls) {
                // 如果该URL在已知的URL集合中已存在，那么不需要再重复抓取。
                if (this.solution.totalUrls.containsKey(url)) {
                    continue;
                }

                this.solution.totalUrls.put(url, true);
            
                String crawlHostName = this.solution.extractHostName(url);
                if (!crawlHostName.equals(this.hostName)) {
                    // 如果抓取的URL对应的HostName同Start URL对应的HostName不同，那么直接丢弃该URL。
                    continue;
                }
                
                // 将该URL添加至结果链表。
                this.solution.addUrlToResult(url);
                // 将该URL添加至待抓取链表，以便进行下一跳抓取。
                this.solution.addUrlToCrawl(url);
            }
        }
    }

    private void addUrlToResult(String url) {
        this.resultLock.lock();
        try {
            this.resultUrls.add(url);
        } finally {
            this.resultLock.unlock();
        }
    }

    private List<String> fetchResultUrls() {
        this.resultLock.lock();
        try {
            return this.resultUrls;
        } finally {
            this.resultLock.unlock();
        }
    }

    private void addUrlToCrawl(String url) {
        this.crawlLock.lock();
        try {
            this.urlsToCrawl.add(url);
        } finally {
            this.crawlLock.unlock();
        }
    }
    
    private String fetchUrlToCrawl() {
        this.crawlLock.lock();
        try {
            return this.urlsToCrawl.poll();
        } finally {
            this.crawlLock.unlock();
        }
    }

    private void incrChore() {
        this.choreCount.incrementAndGet();
    }

    private void decrChore() {
        this.choreCount.decrementAndGet();
    }
}
```

# 4.c++ BlockingQueue
用了BlockingQueue, 维护url访问队列。
麻烦的是处理线程退出，当没有线程在工作并且队列为空时线程退出。
unordered_set需要用锁保护，可以实现成concurrentHashMap效率会高很多。



```
template <typename T>
class BlockingQueue {
 public:
  BlockingQueue() : head_(new Node), tail_(head_) {}
  ~BlockingQueue() { delete tail_; }

  void Push(const T& value) {
    std::lock_guard<std::mutex> lk(tail_mutex_);
    tail_->value = value;
    PushNewTail();
  }

  T& Push(T&& value) {
    std::lock_guard<std::mutex> lk(tail_mutex_);
    tail_->value = std::move(value);
    PushNewTail();
    return tail_->value;
  }

  bool TryPop(T& value) {
    std::lock_guard<std::mutex> lk(head_mutex_);
    if (head_ == GetTail()) {
      return false;
    }

    Node* old_head = head_;
    value = std::move(old_head->value);
    head_ = old_head->next;
    delete old_head;
    return true;
  }

  T Pop() {
    std::unique_lock<std::mutex> lk(head_mutex_);
    cond_var_.wait(lk, [this] { return head_ != GetTail(); });

    Node* old_head = head_;
    T value = std::move(old_head->value);
    head_ = old_head->next;
    delete old_head;
    return value;
  }

  std::shared_ptr<T> PopFor(int millisec) {
    std::unique_lock<std::mutex> lk(head_mutex_);
    cond_var_.wait_for(lk, std::chrono::milliseconds(millisec),
                       [this] { return head_ != GetTail(); });

    if (head_ == GetTail()) {
      return nullptr;
    }

    Node* old_head = head_;
    std::shared_ptr<T> value = std::make_shared<T>(std::move(old_head->value));
    head_ = old_head->next;
    delete old_head;
    return value;
  }

  bool Empty() {
    std::unique_lock<std::mutex> lk(head_mutex_);
    return head_ == GetTail();
  }

 private:
  struct Node;

  void PushNewTail() {
    Node* new_tail = new Node;
    tail_->next = new_tail;
    tail_ = new_tail;
    cond_var_.notify_one();
  }

  Node* GetTail() const {
    std::lock_guard<std::mutex> lk(tail_mutex_);
    return tail_;
  }

  struct Node {
    T value;
    Node* next;

    Node() {}
    Node(const T& v) : value(v), next(nullptr) {}
    Node(T&& v) : value(std::move(v)), next(nullptr) {}
  };

  std::mutex head_mutex_;
  mutable std::mutex tail_mutex_;
  std::condition_variable cond_var_;
  Node* head_;
  Node* tail_;
};

class Solution {
 public:
  std::vector<std::string> crawl(std::string start_url,
                                 HtmlParser html_parser) {
    BlockingQueue<std::string> request_queue;
    std::string host_name = GetHostName(start_url);
    request_queue.Push(start_url);

    int len = std::thread::hardware_concurrency();
    std::thread* threads[len];
    int running_mask = 0;

    std::mutex mutex;
    std::condition_variable cond;

    for (int i = 0; i < len; ++i) {
      std::thread* t = new std::thread([&, i, this] {
        while (true) {
          std::unique_lock<std::mutex> lk(mutex);
          cond.wait(
              lk, [&] { return !request_queue.Empty() || running_mask == 0; });

          if (running_mask == 0 && request_queue.Empty()) {
            return;
          }

          std::string url = request_queue.Pop();
          running_mask |= (1 << i);
          lk.unlock();

          if (GetHostName(url) == host_name && TryInsertURL(url)) {
            std::vector<std::string> urls = html_parser.getUrls(url);
            for (std::string& sub_url : urls) {
              request_queue.Push(sub_url);
              cond.notify_one();
            }
          }

          lk.lock();
          running_mask &= !(1 << i);
          if (running_mask == 0 && request_queue.Empty()) {
            cond.notify_all();
          }
          lk.unlock();
        }
      });
      threads[i] = t;
    }

    for (int i = 0; i < len; ++i) {
      threads[i]->join();
      delete threads[i];
    }

    std::vector<std::string> res;
    for (const std::string& url : url_set_) {
      res.push_back(url);
    }
    return res;
  }

  std::string GetHostName(const std::string& url) {
    std::string parsed_url = url.substr(7, url.size() - 7);
    int index = parsed_url.find_first_of('/');
    return index > 0 ? parsed_url.substr(0, index) : parsed_url;
  }

  bool TryInsertURL(const std::string& url) {
    std::lock_guard<std::mutex> lk(url_set_mutex_);
    if (url_set_.find(url) != url_set_.end()) {
      return false;
    }
    url_set_.insert(url);
    return true;
  }

 private:
  std::unordered_set<std::string> url_set_;
  std::mutex url_set_mutex_;
};
```

