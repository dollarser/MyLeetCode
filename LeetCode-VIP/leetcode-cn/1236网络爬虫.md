# 1236网络爬虫
<p>给定一个链接&nbsp;<code>startUrl</code> 和一个接口&nbsp;<code>HtmlParser</code>&nbsp;，请你实现一个网络爬虫，以实现爬取同&nbsp;<code>startUrl</code>&nbsp;拥有相同&nbsp;<strong>域名标签&nbsp;</strong>的全部链接。该爬虫得到的全部链接可以&nbsp;<strong>任何顺序&nbsp;</strong>返回结果。</p>

<p>你的网络爬虫应当按照如下模式工作：</p>

<ul>
	<li>自链接&nbsp;<code>startUrl</code>&nbsp;开始爬取</li>
	<li>调用&nbsp;<code>HtmlParser.getUrls(url)</code>&nbsp;来获得链接<code>url</code>页面中的全部链接</li>
	<li>同一个链接最多只爬取一次</li>
	<li>只输出&nbsp;<strong>域名&nbsp;</strong>与<strong>&nbsp;</strong><code>startUrl</code>&nbsp;<strong>相同&nbsp;</strong>的链接集合</li>
</ul>

<p><img alt="" src="https://assets.leetcode.com/uploads/2019/08/13/urlhostname.png" style="height: 164px; width: 600px;"></p>

<p>如上所示的一个链接，其域名为&nbsp;<code>example.org</code>。简单起见，你可以假设所有的链接都采用&nbsp;<strong>http协议&nbsp;</strong>并没有指定&nbsp;<strong>端口</strong>。例如，链接&nbsp;<code>http://leetcode.com/problems</code>&nbsp;和&nbsp;<code>http://leetcode.com/contest</code>&nbsp;是同一个域名下的，而链接<code>http://example.org/test</code>&nbsp;和&nbsp;<code>http://example.com/abc</code> 是不在同一域名下的。</p>

<p><code>HtmlParser</code> 接口定义如下：&nbsp;</p>

<pre>interface HtmlParser {
  // 返回给定 url 对应的页面中的全部 url 。
  public List&lt;String&gt; getUrls(String url);
}</pre>

<p>下面是两个实例，用以解释该问题的设计功能，对于自定义测试，你可以使用三个变量&nbsp;&nbsp;<code>urls</code>,&nbsp;<code>edges</code>&nbsp;和&nbsp;<code>startUrl</code>。注意在代码实现中，你只可以访问&nbsp;<code>startUrl</code>&nbsp;，而&nbsp;<code>urls</code>&nbsp;和&nbsp;<code>edges</code>&nbsp;不可以在你的代码中被直接访问。</p>

<p>&nbsp;</p>

<p><strong>示例 1：</strong></p>

<p><img alt="" src="https://assets.leetcode.com/uploads/2019/10/23/sample_2_1497.png" style="height: 300px; width: 610px;"></p>

<pre><strong>输入：
</strong>urls = [
&nbsp; &quot;http://news.yahoo.com&quot;,
&nbsp; &quot;http://news.yahoo.com/news&quot;,
&nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
&nbsp; &quot;http://news.google.com&quot;,
&nbsp; &quot;http://news.yahoo.com/us&quot;
]
edges = [[2,0],[2,1],[3,2],[3,1],[0,4]]
startUrl = &quot;http://news.yahoo.com/news/topics/&quot;
<strong>输出：</strong>[
&nbsp; &quot;http://news.yahoo.com&quot;,
&nbsp; &quot;http://news.yahoo.com/news&quot;,
&nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
&nbsp; &quot;http://news.yahoo.com/us&quot;
]
</pre>

<p><strong>示例 2：</strong></p>

<p><strong><img alt="" src="https://assets.leetcode.com/uploads/2019/10/23/sample_3_1497.png" style="height: 270px; width: 540px;"></strong></p>

<pre><strong>输入：</strong>
urls = [
&nbsp; &quot;http://news.yahoo.com&quot;,
&nbsp; &quot;http://news.yahoo.com/news&quot;,
&nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
&nbsp; &quot;http://news.google.com&quot;
]
edges = [[0,2],[2,1],[3,2],[3,1],[3,0]]
startUrl = &quot;http://news.google.com&quot;
<strong>输入：</strong>[&quot;http://news.google.com&quot;]
<strong>解释：</strong>startUrl 链接到所有其他不共享相同主机名的页面。</pre>

<p>&nbsp;</p>

<p><strong>提示：</strong></p>

<ul>
	<li><code>1 &lt;= urls.length &lt;= 1000</code></li>
	<li><code>1 &lt;= urls[i].length &lt;= 300</code></li>
	<li><code>startUrl</code>&nbsp;为&nbsp;<code>urls</code>&nbsp;中的一个。</li>
	<li>域名标签的长为1到63个字符（包括点），只能包含从&lsquo;a&rsquo;到&lsquo;z&rsquo;的ASCII字母、&lsquo;0&rsquo;到&lsquo;9&rsquo;的数字以及连字符即减号（&lsquo;-&rsquo;）。</li>
	<li>域名标签不会以连字符即减号（&lsquo;-&rsquo;）开头或结尾。</li>
	<li>关于域名有效性的约束可参考:&nbsp;&nbsp;<a href="https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames">https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames</a></li>
	<li>你可以假定url库中不包含重复项。</li>
</ul>
































# 解题:
# 1.平鋪直述的寫法
### 解题思路

透過BFS將hostname相同的url加入，再掃一遍
最後將掃過的url，再跟startUrl比對hostname即得答案

### 代码

```javascript
/**
 * // This is the HtmlParser's API interface.
 * // You should not implement it, or speculate about its implementation
 * function HtmlParser() {
 *
 *		@param {string} url
 *     	@return {string[]}
 *     	this.getUrls = function(url) {
 *      	...
 *     	};
 * };
 */

const getHostName = (url) => {
    url = url.replace(/http[s]*:\/\//, ''); // 去掉 http://, https://
    if (url.includes("/")) {
        const end = url.indexOf('/');
        return url.substring(0, end);
    } else {
        return url;
    }
}

/**
 * @param {string} startUrl
 * @param {HtmlParser} htmlParser
 * @return {string[]}
*/
var crawl = function(startUrl, htmlParser) {

    const queue = [];
    const visited = new Set();

    visited.add(startUrl);
    queue.push(startUrl);

    while(queue.length) {
        const url = queue.shift();
        nextUrls = htmlParser.getUrls(url); // http://news.yahoo.com/news/topics/ => [ 'http://news.yahoo.com', 'http://news.yahoo.com/news' ]

        // console.log(url, nextUrls);
        for(let i = 0; i < nextUrls.length; i++) {
            const nextUrl = nextUrls[i];
            if(!visited.has(nextUrl) && getHostName(nextUrl) === getHostName(url)) {
                visited.add(nextUrl);
                queue.push(nextUrl);
            }
        }
    }

    const ans = [];
    let startHostName = getHostName(startUrl);
    for(const url of visited) {
        if(startHostName === getHostName(url)) {
            ans.push(url);
        }
    }

    return ans;
    
};


```
# 2.1236. 网络爬虫
### 解题思路

其实本题就是一个简单的层序遍历问题，用 BFS 即可解决，但是题目描述比较晦涩。

题目解读：

1、给定一个 startUrl，这个 startUrl 的页面存在若干个链接，可以用 HtmlParser 接口的 getUrls(String url) 方法获取该页面的所有链接；

2、返回能够访问到的、和 startUrl 具有同样的主机名的所有链接。

本题的 BFS 逻辑中涉及到“查看当前链接是否已使用”的操作，建议使用 HashSet 来临时存储，到需要返回的时候再把 set 的数据装入 list 中，可以提升 10 倍的速度，原因在于 HashSet 的查找速度比 ArrayList 快。

### 代码

```java
/**
 * // This is the HtmlParser's API interface.
 * // You should not implement it, or speculate about its implementation
 * interface HtmlParser {
 *     public List<String> getUrls(String url) {}
 * }
 */

import java.util.*;

class Solution {

    public List<String> crawl(String startUrl, HtmlParser htmlParser) {
        Queue<String> queue = new LinkedList<>();
        // 这里用 HashSet 比直接用 ArrayList 快十倍，到最后再装入 list
        HashSet<String> set = new HashSet<>();
        set.add(startUrl);
        queue.add(startUrl);
        while (!queue.isEmpty()) {
            String currentUrl = queue.poll();
            List<String> nextUrls = htmlParser.getUrls(currentUrl);
            for (String nextUrl : nextUrls) {
                if (!set.contains(nextUrl) && getHostName(nextUrl).equals(getHostName(currentUrl))) {
                    // 如果链接没使用过且和 currentUrl 同主机，则有效
                    set.add(nextUrl);
                    queue.add(nextUrl);
                }
            }
        }
        return new ArrayList<>(set);
    }

    /**
     * 获取 url 的主机名
     *
     * @param url 示例：http://leetcode.com/problems
     * @return 示例输出：leetcode.com
     */
    private String getHostName(String url) {
        url = url.substring(7);
        if (url.contains("/")) {
            int end = url.indexOf('/'); // 第一次出现斜杠的索引位置
            return url.substring(0, end);
        } else {
            return url;
        }
    }
}
```
# 3.【dfs】【一个坑测试用例说明域名不同结束】
抽象图一二ij
![image.png](https://pic.leetcode-cn.com/1633089845-iHFcDB-image.png)
```
class Solution {
public:
    vector<string>ans;
    vector<string> crawl(string startUrl, HtmlParser htmlParser) {
        string targetDomain=getDomain(startUrl);
        unordered_set<string>vis;
        vis.insert(startUrl);
        ans.push_back(startUrl);
        dfs(startUrl,htmlParser,targetDomain,vis);
        return ans;
    }
    void dfs(string&curUrl,HtmlParser htmlParser,string&targetDomain,unordered_set<string>&vis){
        vector<string>nextUrls=htmlParser.getUrls(curUrl);
        for(auto&nextUrl:nextUrls){
            if(getDomain(nextUrl)!=targetDomain)continue;
            if(vis.count(nextUrl))continue;
            vis.insert(nextUrl);
            ans.push_back(nextUrl);
            dfs(nextUrl,htmlParser,targetDomain,vis);
        }
    }
    string getDomain(string&s){
        int idx=s.find("/",7);
        if(idx==-1)idx=s.size();
        return s.substr(7,idx-7);
    }
};
```

# 4.【中规中矩】DFS + BFS 搜索
### 解题思路
注意要使用pre-control，先判断，把不符合条件的筛选出去，而不能加进去再出队列或者进入下一层再判断，会TLE。

### 代码

```cpp
/**
 * // This is the HtmlParser's API interface.
 * // You should not implement it, or speculate about its implementation
 * class HtmlParser {
 *   public:
 *     vector<string> getUrls(string url);
 * };
 */

class Solution {
public:
    vector<string> crawl(string startUrl, HtmlParser htmlParser) {
        vector<string> res;
        set<string> visited;
        res.push_back(startUrl);
        visited.insert(startUrl);
        const int startSearchPos = 7; // ignore first 7 chars 'http://'
        string host = startUrl.substr(0, startUrl.find('/', startSearchPos));
        dfs(visited, startUrl, htmlParser, host, res);
        return res;
    }

private:
    void dfs(set<string>& visited, string startUrl, HtmlParser htmlParser, string host, vector<string>& res) {
        auto url = htmlParser.getUrls(startUrl);
        for (int i = 0; i < url.size(); i++) {
            if (url[i].substr(0, host.size()) != host) {
                continue;
            }

            if (visited.count(url[i])) {
                continue;
            }

            visited.insert(url[i]);
            res.push_back(url[i]);
            dfs(visited, url[i], htmlParser, host, res);
        }
    }
};

class Solution2 {
public:
    vector<string> crawl(string startUrl, HtmlParser htmlParser) {
        vector<string> res;
        set<string> visited;
        res.push_back(startUrl);
        visited.insert(startUrl);
        const int startSearchPos = 7; // ignore first 7 chars 'http://'
        string host = startUrl.substr(0, startUrl.find('/', startSearchPos));
        bfs(visited, startUrl, htmlParser, host, res);
        return res;
    }

private:
    void bfs(set<string>& visited, string startUrl, HtmlParser htmlParser, string host, vector<string>& res) {
        queue<string> q{{startUrl}};
        while (!q.empty()) {
            int size = q.size();
            while (size--) {
                startUrl = q.front(); q.pop();
                auto url = htmlParser.getUrls(startUrl);
                for (int i = 0; i < url.size(); i++) {
                    if (url[i].substr(0, host.size()) != host) {
                        continue;
                    }

                    if (visited.count(url[i])) {
                        continue;
                    }

                    visited.insert(url[i]);
                    res.push_back(url[i]);
                    // dfs(visited, url[i], htmlParser, host, res);
                    q.push(url[i]);
                }
            }
        }
    }
};
```
